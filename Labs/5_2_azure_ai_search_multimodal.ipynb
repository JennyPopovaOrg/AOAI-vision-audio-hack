{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Search Multimodal Retrieval\n",
    "\n",
    "\n",
    "As a scenario, this code shows you an approach for text-to-image and image-to-image vector queries. The multimodal embeddings used in this sample are provided by [Azure AI Vision 4.0](https://learn.microsoft.com/azure/ai-services/computer-vision/how-to/image-retrieval) and the [Image Retrieval REST API](https://learn.microsoft.com/rest/api/computervision/image-retrieval) which supports built-in vectorization of images. \n",
    "\n",
    "For indexing, the pattern uses the built in Vision Vectorizer skill to call the Image Retrieval API. Provisioning of this search service, AI Services account, and setup of the indexer is fully automated and included as a step in this notebook.\n",
    "\n",
    "The AI services accounts is also used during queries, as the vectorizer. A vectorizer specifies which embedding model to use for vectorizing a text query string or an images. As always, it's strongly recommended that query vectorization is performed using the same embedding model used for document vectorization during indexing.\n",
    "\n",
    "\n",
    "\n",
    "We use the [Azure Python SDK](https://learn.microsoft.com/en-us/python/api/azure-search-documents/?view=azure-python-preview) for indexer-driven indexing and vector query operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install azure-search-documents --pre --quiet\n",
    "#! pip install openai python-dotenv azure-identity cohere azure-ai-vision-imageanalysis --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "\n",
    "from azure.search.documents.models import (\n",
    "    HybridCountAndFacetMode,\n",
    "    HybridSearch,\n",
    "    SearchScoreThreshold,\n",
    "    VectorizableTextQuery,\n",
    "    VectorizableImageBinaryQuery,\n",
    "    VectorizableImageUrlQuery,\n",
    "    VectorSimilarityThreshold,\n",
    ")\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display, HTML\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Configuration\n",
    "AZURE_AI_VISION_API_KEY = os.getenv(\"AZURE_AI_SERVICES_API_KEY\")\n",
    "AZURE_AI_VISION_ENDPOINT = os.getenv(\"AZURE_AI_SERVICES_ENDPOINT\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "BLOB_CONNECTION_STRING = os.getenv(\"BLOB_CONNECTION_STRING\")\n",
    "BLOB_CONTAINER_NAME = os.getenv(\"AZURE_STORAGE_CONTAINER\")\n",
    "INDEX_NAME = os.getenv(\"AZURE_SEARCH_INDEX\")\n",
    "AZURE_SEARCH_API_KEY = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to **Azure AI Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-specified parameter\n",
    "USE_AAD_FOR_SEARCH = False  # Set this to False to use API key for authentication\n",
    "\n",
    "def authenticate_azure_search(api_key=None, use_aad_for_search=False):\n",
    "    if use_aad_for_search:\n",
    "        print(\"Using AAD for authentication.\")\n",
    "        credential = DefaultAzureCredential()\n",
    "    else:\n",
    "        print(\"Using API keys for authentication.\")\n",
    "        if api_key is None:\n",
    "            raise ValueError(\"API key must be provided if not using AAD for authentication.\")\n",
    "        credential = AzureKeyCredential(api_key)\n",
    "    return credential\n",
    "\n",
    "azure_search_credential = authenticate_azure_search(api_key=AZURE_SEARCH_API_KEY, use_aad_for_search=USE_AAD_FOR_SEARCH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check status of Indexer execution\n",
    "Let's see how indexing process is going on. \n",
    "\n",
    "The result should be: Status: success, Items Processed: 0, Items Failed: 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexerClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "indexer_name = f\"{INDEX_NAME}-indexer\"\n",
    "\n",
    "search_indexer_client = SearchIndexerClient(endpoint=AZURE_SEARCH_ENDPOINT, credential=azure_search_credential)\n",
    "status = search_indexer_client.get_indexer_status(name=indexer_name)\n",
    "print(f\"Status: {status.last_result.status}, Items Processed: {status.last_result.item_count}, Items Failed: {status.last_result.failed_item_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple vector search (text to text)\n",
    "\n",
    "Here we can try text query and execute search against image caption and text vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SearchClient\n",
    "search_client = SearchClient(\n",
    "    AZURE_SEARCH_ENDPOINT,\n",
    "    index_name=INDEX_NAME,\n",
    "    credential=azure_search_credential,\n",
    ")\n",
    "\n",
    "# Define the query\n",
    "# query = \"sunglasses for holiday\"\n",
    "# query = \"休日のサングラス\" # Japanese query\n",
    "query = \"female red dresses\" # English query\n",
    "\n",
    "\n",
    "vector_query = VectorizableTextQuery(\n",
    "    text=query,\n",
    "    k_nearest_neighbors=3,\n",
    "    fields=\"captionVector\",\n",
    "    #fields=\"imageVector\",\n",
    ")\n",
    "\n",
    "# Perform the search\n",
    "results = search_client.search(\n",
    "    search_text=None,\n",
    "    vector_queries=[vector_query],\n",
    "    top=3\n",
    ")\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(BLOB_CONNECTION_STRING)\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"Caption: {result['caption']}\")\n",
    "    print(f\"Score: {result['@search.score']}\")\n",
    "    print(f\"File: {result['metadata_storage_path']}\")\n",
    "    blob_name = os.path.basename(result['metadata_storage_path'])\n",
    "    blob_client = blob_service_client.get_blob_client(container=BLOB_CONTAINER_NAME, blob=blob_name)\n",
    "    display(Image(data=blob_client.download_blob().readall(),  width=200,))\n",
    "    print(\"-\" * 50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Search (text to image)\n",
    "\n",
    "Here we can try text query and execute search against image vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SearchClient\n",
    "search_client = SearchClient(\n",
    "    AZURE_SEARCH_ENDPOINT,\n",
    "    index_name=INDEX_NAME,\n",
    "    credential=azure_search_credential,\n",
    ")\n",
    "\n",
    "# Define the query\n",
    "# query = \"sunglasses for holiday\"\n",
    "# query = \"休日のサングラス\" # Japanese query\n",
    "query = \"female red dresses\" # Spanish query\n",
    "#query = \"Lady in red\" # Spanish query\n",
    "\n",
    "vector_query = VectorizableTextQuery(\n",
    "    text=query,\n",
    "    k_nearest_neighbors=3,\n",
    "    #fields=\"captionVector\",\n",
    "    fields=\"imageVector\",\n",
    ")\n",
    "\n",
    "# Perform the search\n",
    "results = search_client.search(\n",
    "    search_text=None,\n",
    "    vector_queries=[vector_query],\n",
    "    top=3\n",
    ")\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(BLOB_CONNECTION_STRING)\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"Caption: {result['caption']}\")\n",
    "    print(f\"Score: {result['@search.score']}\")\n",
    "    print(f\"File: {result['metadata_storage_path']}\")\n",
    "    blob_name = os.path.basename(result['metadata_storage_path'])\n",
    "    blob_client = blob_service_client.get_blob_client(container=BLOB_CONTAINER_NAME, blob=blob_name)\n",
    "    display(Image(data=blob_client.download_blob().readall(),  width=200,))\n",
    "    print(\"-\" * 50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Vector Search (text to text,  image to image)\n",
    "\n",
    "Here we will execute multimodal query against text and image vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the text query\n",
    "query = \"shoes for running\"\n",
    "url=\"https://images.unsplash.com/photo-1542291026-7eec264c27ff?q=80&w=1770&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\" # Image of a Red Nike Running Shoe\n",
    "\n",
    "text_vector_query = VectorizableTextQuery(\n",
    "    text=query,\n",
    "    k_nearest_neighbors=10,\n",
    "    fields=\"captionVector\",\n",
    ")\n",
    "# Define the image query\n",
    "\n",
    "image_vector_query = VectorizableImageUrlQuery(  # Alternatively, use VectorizableImageBinaryQuery\n",
    "    url=url,    \n",
    "    k_nearest_neighbors=10,\n",
    "    fields=\"imageVector\",\n",
    ")\n",
    "\n",
    "# Perform the search\n",
    "results = search_client.search(\n",
    "    search_text=None, vector_queries=[text_vector_query, image_vector_query], top=3, query_type=\"semantic\", semantic_configuration_name=\"default\"\n",
    ")\n",
    "\n",
    "print(\"Image input to search:\")\n",
    "display(Image(url=url, width=200))\n",
    "print(\"Search results:\")\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"Caption: {result['caption']}\")\n",
    "    print(f\"Score: {result['@search.score']}\")\n",
    "    print(f\"File: {result['metadata_storage_path']}\")\n",
    "    blob_name = os.path.basename(result['metadata_storage_path'])\n",
    "    blob_client = blob_service_client.get_blob_client(container=BLOB_CONTAINER_NAME, blob=blob_name)\n",
    "    display(Image(data=blob_client.download_blob().readall(),  width=200,))\n",
    "    print(\"-\" * 50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal Vector Search (text to image,  image to text)\n",
    "\n",
    "It is even possible to run query using image input to text vector and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the text query\n",
    "query = \"shoes for running\"\n",
    "url=\"https://images.unsplash.com/photo-1542291026-7eec264c27ff?q=80&w=1770&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\" # Image of a Red Nike Running Shoe\n",
    "\n",
    "text_vector_query = VectorizableTextQuery(\n",
    "    text=query,\n",
    "    k_nearest_neighbors=10,\n",
    "    #fields=\"captionVector\",\n",
    "     fields=\"imageVector\",\n",
    ")\n",
    "# Define the image query\n",
    "image_vector_query = VectorizableImageUrlQuery(  # Alternatively, use VectorizableImageBinaryQuery\n",
    "    url=url,\n",
    "    k_nearest_neighbors=10,\n",
    "    #fields=\"imageVector\",\n",
    "    fields=\"captionVector\",\n",
    ")\n",
    "\n",
    "# Perform the search\n",
    "results = search_client.search(\n",
    "    search_text=None, vector_queries=[text_vector_query, image_vector_query], top=3\n",
    ")\n",
    "\n",
    "print(\"Image input to search:\")\n",
    "display(Image(url=url, width=200))\n",
    "print(\"Search results:\")\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"Caption: {result['caption']}\")\n",
    "    print(f\"Score: {result['@search.score']}\")\n",
    "    print(f\"File: {result['metadata_storage_path']}\")\n",
    "    blob_name = os.path.basename(result['metadata_storage_path'])\n",
    "    blob_client = blob_service_client.get_blob_client(container=BLOB_CONTAINER_NAME, blob=blob_name)\n",
    "    display(Image(data=blob_client.download_blob().readall(),  width=200,))\n",
    "    print(\"-\" * 50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-modal vector search with weighting images 100x more than captions\n",
    "\n",
    "It is possible to weight search results differently, for example, rely 100 times more on Image search than on Text search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the text query\n",
    "query = \"shoes for running\"\n",
    "url=\"https://images.unsplash.com/photo-1542291026-7eec264c27ff?q=80&w=1770&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\"  # Image of a Red Nike Running Shoe\n",
    "\n",
    "text_vector_query = VectorizableTextQuery(\n",
    "    text=query,\n",
    "    k_nearest_neighbors=5,\n",
    "    fields=\"captionVector\",\n",
    ")\n",
    "# Define the image query\n",
    "image_vector_query = VectorizableImageUrlQuery(  # Alternatively, use VectorizableImageBinaryQuery\n",
    "    url=url,\n",
    "    k_nearest_neighbors=5,\n",
    "    fields=\"imageVector\",\n",
    "    weight=100,\n",
    ")\n",
    "\n",
    "# Perform the search\n",
    "results = search_client.search(\n",
    "    search_text=None, vector_queries=[text_vector_query, image_vector_query], top=3\n",
    ")\n",
    "\n",
    "print(\"Image input to search:\")\n",
    "display(Image(url=url, width=200))\n",
    "print(\"Search results:\")\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"Caption: {result['caption']}\")\n",
    "    print(f\"Score: {result['@search.score']}\")\n",
    "    print(f\"File: {result['metadata_storage_path']}\")\n",
    "    blob_name = os.path.basename(result['metadata_storage_path'])\n",
    "    blob_client = blob_service_client.get_blob_client(container=BLOB_CONTAINER_NAME, blob=blob_name)\n",
    "    display(Image(data=blob_client.download_blob().readall(),  width=200,))\n",
    "    print(\"-\" * 50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "Try different combination of search retrieval with your own images and search queries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
