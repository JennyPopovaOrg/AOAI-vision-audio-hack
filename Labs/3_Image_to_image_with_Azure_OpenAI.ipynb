{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "621eba3b",
   "metadata": {},
   "source": [
    "# Image to image with Azure Open AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e0641c",
   "metadata": {},
   "source": [
    "In this lab we are going to use **Azure AI Vision service** and **Azure OprnAI GPT-4o model** to get reach description for an image.\n",
    "Then we will use this description as a prompt to generate an artificial image using **Azure Open AI and its Dall-e-3** model.\n",
    "\n",
    "You will be able to compare qulaty and precision of images created based on different type of description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d830966",
   "metadata": {
    "gather": {
     "logged": 1683118303834
    }
   },
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import datetime\n",
    "import openai\n",
    "import os\n",
    "import requests\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "from IPython.display import Image as viewimage\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbe4ff3",
   "metadata": {},
   "source": [
    "Load environmrnt variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2fe1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "# Azure Open AI\n",
    "openai.api_type: str = \"azure\"\n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai.api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "model =   os.getenv(\"AZURE_OPENAI_MODEL\")  # This is the deployed name of your GPT4o model from the Azure Open AI studio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d853ab37",
   "metadata": {},
   "source": [
    "Create a folder for generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6a531e",
   "metadata": {
    "gather": {
     "logged": 1683118304308
    }
   },
   "outputs": [],
   "source": [
    "RESULTS_DIR = \"../data/artificial_images\"\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0bf737",
   "metadata": {},
   "source": [
    "## 1. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b553b9ff",
   "metadata": {},
   "source": [
    " - Function do create a reach image description using **Azure AI Vision** service.\n",
    " We will use denseCaption option. Note that in classical image analysis, you cannot define where model shall focus when creating an image description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e9073",
   "metadata": {
    "gather": {
     "logged": 1683118310084
    }
   },
   "outputs": [],
   "source": [
    "def get_caption(image_file):\n",
    "    \"\"\"\n",
    "    Get caption from an image using Azure Computer Vision 4\n",
    "    \"\"\"\n",
    "    # Get key and endpoint for Azure Computer Vision service\n",
    "    load_dotenv()\n",
    "    key = os.getenv(\"AZURE_AI_SERVICES_API_KEY\")\n",
    "    endpoint = os.getenv(\"AZURE_AI_SERVICES_ENDPOINT\")\n",
    "\n",
    "    # settings\n",
    "    #options = \"&features=caption,tags\"\n",
    "    options = \"&features=denseCaptions,tags\" \n",
    "    model = \"?api-version=2023-02-01-preview&modelVersion=latest\"\n",
    "    url = endpoint + \"/computervision/imageanalysis:analyze\" + model + options\n",
    "    headers = {\n",
    "        \"Content-type\": \"application/octet-stream\",\n",
    "        \"Ocp-Apim-Subscription-Key\": key,\n",
    "    }\n",
    "\n",
    "    # Read the image file\n",
    "    with open(image_file, \"rb\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Sending the requests\n",
    "    response = requests.post(url, data=data, headers=headers)\n",
    "    results = response.json()\n",
    "\n",
    "    # Parsing the results\n",
    "    #image_caption = results[\"captionResult\"][\"text\"]\n",
    "    image_caption = \"; \".join(item[\"text\"] for item in results[\"denseCaptionsResult\"][\"values\"])\n",
    "\n",
    "    tags = results[\"tagsResult\"][\"values\"]\n",
    "    tags_string = \", \".join(item[\"name\"] for item in tags)\n",
    "    caption = image_caption + \", \" + tags_string\n",
    "\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b97c919",
   "metadata": {},
   "source": [
    "- Function to creage a reach image description with focus on fashion features with **Azure OpenAI GPT-4o** model\n",
    "\n",
    "Here you can define what kind of description you want to get from a picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb344b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "model =   os.getenv(\"AZURE_OPENAI_MODEL\")\n",
    "\n",
    "def gpt4V_fashion(image_file):\n",
    "    \"\"\"\n",
    "    GPT4-Vision\n",
    "    \"\"\"\n",
    "    # Checking if file exists\n",
    "    if not os.path.exists(image_file):\n",
    "        print(f\"[Error] Image file {image_file} does not exist.\")\n",
    "        return None\n",
    "\n",
    "    # Endpoint\n",
    "    base_url = f\"{openai.api_base}/openai/deployments/{model}\"\n",
    "    gpt4vision_endpoint = f\"{base_url}/chat/completions?api-version={openai.api_version}\"\n",
    "\n",
    "\n",
    "    # Header\n",
    "    headers = {\"Content-Type\": \"application/json\", \"api-key\": openai.api_key}\n",
    "\n",
    "    # Encoded image\n",
    "    encoded_image = base64.b64encode(open(image_file, \"rb\").read()).decode(\n",
    "        \"ascii\"\n",
    "    )\n",
    "\n",
    "    context = \"\"\" \n",
    "    You are a fashion expert, familiar with identifying features of fashion articles from images.\n",
    "    A user uploads an image and asks you to describe one particular piece in the shot: jacket, shoes, pants, \\\n",
    "    watches, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    You respond with your analysis of the following fields:\n",
    "\n",
    "    1. ITEM'S TYPE: Identify if it's a top, bottom, dress, outerwear, footwear, bag, jewelry...\n",
    "    2. BRAND: identity the brand of the item.\n",
    "    3. COLOR: Note the main color(s) and any secondary colors.\n",
    "    4. PATTERN: Identify any visible patterns such as stripes, florals, animal print, or geometric designs.\\\n",
    "    Feel free to use any other patterns here.\n",
    "    5. MATERIAL: Best guess at the material that the item is made from.\n",
    "    6. FEATURES: Note any unique details or embellishments, like embroidery, sequins, studs, fringes, buttons,\n",
    "    zippers...\n",
    "    7. ITEM TYPE SPECIFIC: For each type of item, feel free to add any additional descriptions that are relevant \\\n",
    "    to help describe the item. For example, for a jacket you can include the neck and sleeve design, plus the length.\n",
    "    8. MISC.: Anything else important that you notice.\n",
    "    9. SIZE: Print the size of the item if you get it from the image.\n",
    "    10. ITEM SUMMARY: Write a one line summary for this item.\n",
    "    11. ITEM CLASSIFICATION: Classify this item into CLOTHES, BAG, SHOES, WATCH or OTHERS.\n",
    "    12. ITEM TAGS: Generate 10 tags to describe this item. Each tags should be separated with a comma.\n",
    "    13. STORIES: Write multiple stories about this product in 5 lines.\n",
    "\n",
    "    The output should be a numbered bulleted list. Just print an empty line between each items starting at item 12.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    json_data = {\n",
    "        \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": context\n",
    "        }\n",
    "      ]\n",
    "    }, \n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": prompt\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "        \"max_tokens\": 4000,\n",
    "        \"temperature\": 0.7,\n",
    "    }\n",
    "\n",
    "    # Results\n",
    "    response = requests.post(\n",
    "        gpt4vision_endpoint, headers=headers, data=json.dumps(json_data)\n",
    "    )\n",
    "\n",
    "    if response.status_code == 200:\n",
    "           return json.loads(response.text)[\"choices\"][0][\"message\"][\"content\"]\n",
    "  \n",
    "    elif response.status_code == 429:\n",
    "        print(\n",
    "            \"[429 Error] Too many requests. Please wait a couple of seconds and try again.\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"[Error] Error code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2721c88c",
   "metadata": {},
   "source": [
    "- Function to generate image with **Azure OpenAI Dall-e-3** model\n",
    "\n",
    "Here we will send image description as a prompt to Dall-e-3 model to generate an image. Generate images will be stored in **data/artificial_images** folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9908b53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from openai import AzureOpenAI\n",
    "import json\n",
    "\n",
    "def dalle3(prompt, image_file, size=1024):\n",
    "    \"\"\"\n",
    "    Generate an image from a prompt with Dall e 2\n",
    "    \"\"\"\n",
    "    # prompt\n",
    "    extra_prompt = \"full view, detailled, 8K, do not generate any provocative content\"\n",
    "    prompt = prompt + extra_prompt\n",
    "\n",
    "    # Get the endpoint and key for Azure Open AI\n",
    "    load_dotenv(\"azure.env\")\n",
    "    openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "    api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\") or \"2024-05-01-preview\"\n",
    "#-----------------\n",
    "# Note: DALL-E 3 requires version 1.0.0 of the openai-python library or later\n",
    "\n",
    "\n",
    "    openai_client = openai.AzureOpenAI(\n",
    "            api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\") or \"2024-02-15-preview\",\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    result = openai_client.images.generate(\n",
    "        model=\"dall-e-3\", # the name of your DALL-E 3 deployment\n",
    "        prompt=prompt,\n",
    "        n=1, \n",
    "        size = \"1024x1024\"\n",
    "    )\n",
    "\n",
    "    image_url = json.loads(result.model_dump_json())['data'][0]['url']\n",
    "    #-----------------\n",
    "\n",
    "    response = requests.get(image_url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "\n",
    "        # Saving the artificial image\n",
    "    filename = os.path.basename(image_file)\n",
    "    output_file = os.path.join(RESULTS_DIR, \"dalle3_\" + filename)\n",
    "    img.save(output_file)\n",
    "\n",
    "    return img, output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dad5152",
   "metadata": {},
   "source": [
    "Function to display 2 images side by side "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7f4ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(imagefile1, imagefile2):\n",
    "    \"\"\"\n",
    "    Display two images side by side\n",
    "    \"\"\"\n",
    "    # Reading images\n",
    "    img1 = plt.imread(imagefile1)\n",
    "    img2 = plt.imread(imagefile2)\n",
    "\n",
    "    # Subplots\n",
    "    f, ax = plt.subplots(1, 2, figsize=(15, 8))\n",
    "    ax[0].imshow(img1)\n",
    "    ax[1].imshow(img2)\n",
    "\n",
    "    # Titles\n",
    "    ax[0].set_title(\"Original image\")\n",
    "    ax[1].set_title(\"DALL·E 3 artificial image\")\n",
    "    title = \"Image to image with Azure Open AI\"\n",
    "    f.suptitle(title, fontsize=15)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Removing the axis\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[1].axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efdae0e",
   "metadata": {},
   "source": [
    "## 2. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e17ab",
   "metadata": {},
   "source": [
    "#### Test 1. With image description from **Azure AI Vision**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aad111",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file='../data/fashion/image1.jpg'\n",
    "\n",
    "print(image_file)\n",
    "viewimage(filename=image_file, width=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d1bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = get_caption(image_file)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b320861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_img, artificial_image = dalle3(prompt, image_file)\n",
    "\n",
    "display_images(image_file, artificial_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3431f09a",
   "metadata": {},
   "source": [
    "#### Test 2. With reach fashion caption from **Azure OpenAI GPT-4o**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee66358",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=gpt4V_fashion(image_file)\n",
    "\n",
    "artificial_img, artificial_image = dalle3(prompt, image_file)\n",
    "display_images(image_file, artificial_image)\n",
    "print (prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfefa147",
   "metadata": {},
   "source": [
    "## 2. Challenge\n",
    "\n",
    "1. Generate artificial images for other examples in data/fashion folder based on simple or reach description \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca18aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = \"...\" # path to the image file\n",
    "#image_view(image_file)1\n",
    "prompt=... # call gpt4V_fashion(image_file) or get_caption(image_file) to get image description. Alternatively provide your own prompt\n",
    "\n",
    "\n",
    "artificial_img, artificial_image = dalle3(prompt, image_file)\n",
    "display_images(image_file, artificial_image)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
