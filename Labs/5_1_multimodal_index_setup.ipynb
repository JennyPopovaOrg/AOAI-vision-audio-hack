{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview\n",
    "In this lab, we will show you how to create a multimodal (text + images) vector index in **Azure AI Search.**\n",
    "\n",
    "### Prerequisites\n",
    "- üêç Python 3.9 or higher\n",
    "- ‚òÅÔ∏è [Azure Blob storage](https://learn.microsoft.com/azure/storage/common/storage-account-create), used as the data source during indexing.\n",
    "- üîó Azure AI Vision Service or Azure AI Multi-Service Account (https://learn.microsoft.com/azure/ai-services/multi-service-resource), used for multi-modal embeddings.\n",
    "- üîó [Azure AI Search](https://learn.microsoft.com/azure/search/search-create-service-portal), any region and tier, but we recommend Basic or higher for this workload.\n",
    "\n",
    "We use the [Azure Python SDK](https://learn.microsoft.com/en-us/python/api/azure-search-documents/?view=azure-python-preview) for indexer-driven indexing and vector query operations.\n",
    "\n",
    "For indexing, the pattern uses the built in **Vision Vectorizer skill** to call the Image Retrieval API. Provisioning of this search service, **AI Services** account, and setup of the indexer is fully automated and included as a step in this notebook.\n",
    "\n",
    "The AI services accounts is also used during queries, as the vectorizer. A vectorizer specifies which embedding model to use for vectorizing a text query string or an images. As always, it's strongly recommended that query vectorization is performed using the same embedding model used for document vectorization during indexing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    AIServicesVisionParameters,\n",
    "    AIServicesVisionVectorizer,\n",
    "    BlobIndexerImageAction,\n",
    "    CognitiveServicesAccountKey,\n",
    "    FieldMapping,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    ImageAnalysisSkill,\n",
    "    IndexerExecutionStatus,\n",
    "    IndexingParameters,\n",
    "    IndexingParametersConfiguration,\n",
    "    InputFieldMappingEntry,\n",
    "    OutputFieldMappingEntry,\n",
    "    ScalarQuantizationCompressionConfiguration,\n",
    "    ScalarQuantizationParameters,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndex,\n",
    "    SearchIndexer,\n",
    "    SearchIndexerDataContainer,\n",
    "    SearchIndexerDataSourceConnection,\n",
    "    SearchIndexerSkillset,\n",
    "    SemanticConfiguration,\n",
    "    SemanticField,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticSearch,\n",
    "    SimpleField,\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchAlgorithmMetric,\n",
    "    VectorSearchProfile,\n",
    "    VisionVectorizeSkill,\n",
    "    MergeSkill\n",
    ")\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display, HTML\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Configuration\n",
    "AZURE_AI_VISION_API_KEY = os.getenv(\"AZURE_AI_SERVICES_API_KEY\")\n",
    "AZURE_AI_VISION_ENDPOINT = os.getenv(\"AZURE_AI_SERVICES_ENDPOINT\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "BLOB_CONNECTION_STRING = os.getenv(\"BLOB_CONNECTION_STRING\")\n",
    "BLOB_CONTAINER_NAME = os.getenv(\"AZURE_STORAGE_CONTAINER\")\n",
    "INDEX_NAME = os.getenv(\"AZURE_SEARCH_INDEX\")\n",
    "AZURE_SEARCH_API_KEY = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Upload sample data to blob container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Azure Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "\n",
    "# Get environment variables for Azure AI Vision\n",
    "try:\n",
    "    connection_string = BLOB_CONNECTION_STRING\n",
    "    # container_name = os.getenv(\"BLOB_CONTAINER_NAME\")\n",
    "    container_name = BLOB_CONTAINER_NAME\n",
    "except KeyError as e:\n",
    "    print(f\"Missing environment variable: {str(e)}\")\n",
    "    print(\"Set them before running this sample.\")\n",
    "    exit()\n",
    "\n",
    "# Setup for Azure Blob Storage\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_client = blob_service_client.get_container_client(container_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data/sample data to blob\n",
    "Here we are uploading our data/samples images to **Azure Blob Storage** so they can be accessible by **Azure AI Search** for indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_folder_path = \"../data/samples\"\n",
    "\n",
    "try:\n",
    "    container_client.create_container()\n",
    "except Exception as e:\n",
    "    print(f\"Container already exists: {e}\")\n",
    "    pass\n",
    "\n",
    "# Upload files from the local folder to the blob container\n",
    "for root, dirs, files in os.walk(local_folder_path):\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(root, file_name)\n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=file_name)\n",
    "        \n",
    "        with open(file_path, \"rb\") as data:\n",
    "            blob_client.upload_blob(data, overwrite=True)\n",
    "            print(f\"Uploaded {file_name} to {container_name}\")\n",
    "\n",
    "print(\"All files uploaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create multimodal index in **Azure AI Search**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Azure AI Serch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-specified parameter\n",
    "USE_AAD_FOR_SEARCH = False  # Set this to False to use API key for authentication\n",
    "\n",
    "def authenticate_azure_search(api_key=None, use_aad_for_search=False):\n",
    "    if use_aad_for_search:\n",
    "        print(\"Using AAD for authentication.\")\n",
    "        credential = DefaultAzureCredential()\n",
    "    else:\n",
    "        print(\"Using API keys for authentication.\")\n",
    "        if api_key is None:\n",
    "            raise ValueError(\"API key must be provided if not using AAD for authentication.\")\n",
    "        credential = AzureKeyCredential(api_key)\n",
    "    return credential\n",
    "\n",
    "azure_search_credential = authenticate_azure_search(api_key=AZURE_SEARCH_API_KEY, use_aad_for_search=USE_AAD_FOR_SEARCH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a blob data source connector on Azure AI Search\n",
    "\n",
    "Datasource in Azure Ai search is pointing to a place where images are located. In our case it will be a blob storage where we upploaded our data earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_update_data_source(indexer_client, container_name, connection_string, index_name):\n",
    "    \"\"\"\n",
    "    Create or update a data source connection for Azure AI Search.\n",
    "    \"\"\"\n",
    "    container = SearchIndexerDataContainer(name=container_name)\n",
    "    data_source_connection = SearchIndexerDataSourceConnection(\n",
    "        name=f\"{index_name}-blob\",\n",
    "        type=\"azureblob\",\n",
    "        connection_string=connection_string,\n",
    "        container=container\n",
    "    )\n",
    "    try:\n",
    "        indexer_client.create_or_update_data_source_connection(data_source_connection)\n",
    "        print(f\"Data source '{index_name}-blob' created or updated successfully.\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to create or update data source due to error: {e}\")\n",
    "\n",
    "# Create a SearchIndexerClient instance\n",
    "indexer_client = SearchIndexerClient(AZURE_SEARCH_ENDPOINT, azure_search_credential)\n",
    "\n",
    "# Call the function to create or update the data source\n",
    "create_or_update_data_source(indexer_client, BLOB_CONTAINER_NAME, BLOB_CONNECTION_STRING, INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a search index\n",
    "\n",
    "Here we will define our multimodal search index and all necessery components. \n",
    "Our index will contain the following fields to search\n",
    "- text field with image caption \n",
    "- vector fields with caption text vector\n",
    "- vector field with image vector\n",
    "- field with image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fields():\n",
    "    # Creates the fields for the search index based on the specified schema.\n",
    "    return [\n",
    "        SimpleField(\n",
    "            name=\"id\", type=SearchFieldDataType.String, key=True, filterable=True\n",
    "        ),\n",
    "        SearchField(name=\"caption\", type=SearchFieldDataType.String, searchable=True), # image caption\n",
    "        SearchField(name=\"metadata_storage_path\", type=SearchFieldDataType.String, searchable=True), # image path\n",
    "        SearchField(\n",
    "            name=\"captionVector\", # vectorized caption\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            vector_search_dimensions=1024,\n",
    "            vector_search_profile_name=\"myHnswProfile\",\n",
    "            stored=False,\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"imageVector\", # vectorized image\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            vector_search_dimensions=1024,\n",
    "            vector_search_profile_name=\"myHnswProfile\",\n",
    "            stored=False,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_vector_search_configuration():\n",
    "    # Creates the vector search configuration for the search index.\n",
    "    # In the configuration we specify the algorithms, compressions, vectorizers to vectorise our queries and execute search\n",
    "\n",
    "    return VectorSearch(\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"myHnsw\",\n",
    "                parameters=HnswParameters(\n",
    "                    m=4,\n",
    "                    ef_construction=400,\n",
    "                    ef_search=500,\n",
    "                    metric=VectorSearchAlgorithmMetric.COSINE,\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "        compressions=[\n",
    "            ScalarQuantizationCompressionConfiguration(\n",
    "                name=\"myScalarQuantization\",\n",
    "                rerank_with_original_vectors=True,\n",
    "                default_oversampling=10,\n",
    "                parameters=ScalarQuantizationParameters(quantized_data_type=\"int8\"),\n",
    "            )\n",
    "        ],\n",
    "        vectorizers=[\n",
    "            AIServicesVisionVectorizer(\n",
    "                name=\"myAIServicesVectorizer\",\n",
    "                kind=\"aiServicesVision\",\n",
    "                ai_services_vision_parameters=AIServicesVisionParameters(\n",
    "                    model_version=\"2023-04-15\",\n",
    "                    resource_uri=AZURE_AI_VISION_ENDPOINT,\n",
    "                    api_key=AZURE_AI_VISION_API_KEY,\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"myHnswProfile\",\n",
    "                algorithm_configuration_name=\"myHnsw\",\n",
    "                compression_configuration_name=\"myScalarQuantization\",\n",
    "                vectorizer=\"myAIServicesVectorizer\",\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "# Creating an index with the specified fields and vector search configuration\n",
    "def create_search_index(index_client, index_name, fields, vector_search):\n",
    "    \"\"\"Creates or updates a search index.\"\"\"\n",
    "    index = SearchIndex(\n",
    "        name=index_name,\n",
    "        fields=fields,\n",
    "        vector_search=vector_search,\n",
    "    )\n",
    "    index_client.create_or_update_index(index=index)\n",
    "\n",
    "# Create a SearchIndexClient instance for further operations\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=AZURE_SEARCH_ENDPOINT, credential=azure_search_credential\n",
    ")\n",
    "fields = create_fields()\n",
    "vector_search = create_vector_search_configuration()\n",
    "\n",
    "# Create the search index with the adjusted schema\n",
    "create_search_index(index_client, INDEX_NAME, fields, vector_search)\n",
    "print(f\"Created index: {INDEX_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Skillset   \n",
    "\n",
    "Skillset is a collection of cognitive skills that are applied to your data during the indexing process. These skills can include a variety of AI-powered capabilities such as natural language processing, image analysis, and custom machine learning models. The purpose of a skillset is to enrich your data, extracting useful information and transforming it into a searchable format.\n",
    "\n",
    "In our case we need skills for:\n",
    "- creating an image caption from an image\n",
    "- creating text embedding from the caption\n",
    "- creating image embedding\n",
    "- additional merge skill to flatten complex text data that may include many captions and tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_caption_skill(): # Create a skill to generate image caption\n",
    "    return ImageAnalysisSkill(\n",
    "        name=\"image-description-skill\",\n",
    "        description=\"Skill to generate caption for image\",\n",
    "        context=\"/document/normalized_images/*\",\n",
    "        inputs=[InputFieldMappingEntry(name=\"image\", source=\"/document/normalized_images/0\")],\n",
    "        outputs=[OutputFieldMappingEntry(name=\"description\", target_name=\"description\")],\n",
    "        defaultLanguageCode = \"en\",\n",
    "        visualFeatures = [\"Description\"],\n",
    "    )\n",
    "\n",
    "def create_merge_skill(): # Create a skill to merge text and tags from image caption\n",
    "    return MergeSkill(\n",
    "        name=\"text-merge-skill\",\n",
    "        description=\"merge text and tags from image caption\",\n",
    "        context=\"/document/normalized_images/*\",\n",
    "        #inputs=[InputFieldMappingEntry(name=\"text\", source=\"/document/normalized_images/*/description/captions/0/text\"), InputFieldMappingEntry(name=\"itemsToInsert\", source=\"/document/normalized_images/0/description/tags\")],\n",
    "        inputs=[InputFieldMappingEntry(name=\"itemsToInsert\", source=\"/document/normalized_images/0/description/captions/*/text\")], \n",
    "        outputs=[OutputFieldMappingEntry(name=\"mergedText\", target_name=\"caption\")],\n",
    "        insertPreTag=\", \",\n",
    "        insertPostTag=\"\"\n",
    "    )\n",
    "\n",
    " \n",
    "\n",
    "def create_text_embedding_skill(): # Create a skill to generate embeddings for text\n",
    "    return VisionVectorizeSkill(\n",
    "        name=\"text-embedding-skill\",\n",
    "        description=\"Skill to generate embeddings for text via Azure AI Vision\",\n",
    "        context=\"/document/normalized_images/*\",\n",
    "        model_version=\"2023-04-15\",\n",
    "        inputs=[InputFieldMappingEntry(name=\"text\", source=\"/document/normalized_images/0/caption\")],\n",
    "        outputs=[OutputFieldMappingEntry(name=\"vector\", target_name=\"captionVector\")],\n",
    "    )\n",
    "\n",
    "def create_image_embedding_skill(): # Create a skill to generate embeddings for image\n",
    "    return VisionVectorizeSkill(\n",
    "        name=\"image-embedding-skill\",\n",
    "        description=\"Skill to generate embeddings for image via Azure AI Vision\",\n",
    "        context=\"/document/normalized_images/*\",\n",
    "        model_version=\"2023-04-15\",\n",
    "        inputs=[InputFieldMappingEntry(name=\"image\", source=\"/document/normalized_images/*\")],\n",
    "        outputs=[OutputFieldMappingEntry(name=\"vector\", target_name=\"imageVector\")],\n",
    "    )\n",
    "\n",
    "# Create a skillset with the specified skills\n",
    "def create_skillset(client, skillset_name, image_caption_skill, merge_skill, text_embedding_skill, image_embedding_skill):\n",
    "    skillset = SearchIndexerSkillset(\n",
    "        name=skillset_name,\n",
    "        description=\"Skillset for generating embeddings\",\n",
    "        skills=[image_caption_skill, merge_skill, text_embedding_skill, image_embedding_skill],\n",
    "        cognitive_services_account=CognitiveServicesAccountKey(\n",
    "            key=AZURE_AI_VISION_API_KEY,\n",
    "            description=\"AI Vision Multi Service Account\",\n",
    "        ),\n",
    "    )\n",
    "    client.create_or_update_skillset(skillset)\n",
    "\n",
    "\n",
    "# Create a skillset with the specified skills\n",
    "skillset_name = f\"{INDEX_NAME}-skillset\"\n",
    "image_caption_skill = create_image_caption_skill()\n",
    "text_embedding_skill = create_text_embedding_skill()\n",
    "image_embedding_skill = create_image_embedding_skill()\n",
    "merge_skill = create_merge_skill()\n",
    "\n",
    "create_skillset(indexer_client, skillset_name, image_caption_skill, merge_skill, text_embedding_skill, image_embedding_skill)\n",
    "print(f\"Created skillset: {skillset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Indexer\n",
    "\n",
    "Now, when we have defined all index components\n",
    "- index description \n",
    "- index configuration\n",
    "- skills to enrich data\n",
    "\n",
    "we can execute the Indexer and finally build a Search Index to look up images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an indexer that defins how to index the data and map the fields\n",
    "def create_and_run_indexer(indexer_client, indexer_name, skillset_name, index_name, data_source_name):\n",
    "    indexer = SearchIndexer(\n",
    "        name=indexer_name,\n",
    "        description=\"Indexer to index documents and generate embeddings\",\n",
    "        skillset_name=skillset_name,\n",
    "        target_index_name=index_name,\n",
    "        data_source_name=data_source_name,\n",
    "        parameters=IndexingParameters(\n",
    "            configuration=IndexingParametersConfiguration(\n",
    "                #parsing_mode=BlobIndexerParsingMode.JSON_ARRAY,\n",
    "                image_action=BlobIndexerImageAction.GENERATE_NORMALIZED_IMAGES,\n",
    "                query_timeout=None,\n",
    "            ),\n",
    "        ),\n",
    "        #field_mappings=[FieldMapping(source_field_name=\"id\", target_field_name=\"id\")],\n",
    "        output_field_mappings=[\n",
    "            FieldMapping(source_field_name=\"/document/normalized_images/*/caption\", target_field_name=\"caption\"),\n",
    "            FieldMapping(source_field_name=\"/document/normalized_images/0/captionVector\", target_field_name=\"captionVector\"),\n",
    "            FieldMapping(source_field_name=\"/document/normalized_images/0/imageVector\", target_field_name=\"imageVector\"),\n",
    "            FieldMapping(source_field_name=\"/document/metadata_storage_path\", target_field_name=\"metadata_storage_path\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    indexer_client.create_or_update_indexer(indexer)\n",
    "    print(f\"{indexer_name} created or updated.\")\n",
    "\n",
    "    indexer_client.run_indexer(indexer_name)\n",
    "    print(f\"{indexer_name} is running. If queries return no results, please wait a bit and try again.\")\n",
    "\n",
    "\n",
    "# Run the indexer\n",
    "data_source_name = f\"{INDEX_NAME}-blob\"\n",
    "indexer_name = f\"{INDEX_NAME}-indexer\"\n",
    "\n",
    "create_and_run_indexer(indexer_client, indexer_name, skillset_name, INDEX_NAME, data_source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building an index will take several minutes. \n",
    "Wait a bit and move to exploring different search option for our multimodal index in [Lab 5.2](./5_2_azure_ai_search_multimodal.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
